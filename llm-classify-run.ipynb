{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ddb2d9fc-0f56-4582-b75c-8403cdd30022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  8 17:03:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:42:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             40W /  250W |   14905MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d153ff6e-55f6-466c-8d50-d91ba5eaae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers accelerate datasets optimum peft auto-gptq bitsandbytes scikit-learn torch matplotlib tqdm --quiet\n",
    "!pip install flash-attn --no-build-isolation --quiet # for qwen2, newer nvidia gpus i.e. ampere and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a96cea69-0725-40d6-891c-56327701f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UTAustin-AIHealth/MedHallu dataset (pqa_labeled)...\n",
      "Limiting dataset to 1000 samples for testing.\n",
      "Loading tokenizer for Qwen/Qwen2.5-7B-Instruct...\n",
      "Setting up Qwen/Qwen2.5-7B-Instruct with quantization: None...\n",
      "Loading uncompressed model\n",
      "Loading model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9523d4442d734065b337cc7feb7e72a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen2.5-7B-Instruct model loaded successfully!\n",
      "Preparing prompts\n",
      "Prompts are prepared.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GPTQConfig, BitsAndBytesConfig\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, ConfusionMatrixDisplay #,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc # garbage collector interface\n",
    "import pprint\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-7B-Instruct' # try different models later\n",
    "DATASET_NAME = 'UTAustin-AIHealth/MedHallu'\n",
    "DATASET_CONFIG = 'pqa_labeled' # try adding pqa_artiifcal later\n",
    "BATCH_SIZE = 8\n",
    "MAX_SAMPLES = 1000\n",
    "QUANTIZATION_MODE = None\n",
    "OUTPUT_FILENAME = f'hallucination_results_{MODEL_NAME}_{QUANTIZATION_MODE}_{DATASET_CONFIG}.csv'\n",
    "\n",
    "# --- Load Dataset ---\n",
    "print(f'Loading {DATASET_NAME} dataset ({DATASET_CONFIG})...')\n",
    "ds = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "dataset = ds['train'] # use train split which has 1k labelled samples\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    print(f'Limiting dataset to {MAX_SAMPLES} samples for testing.')\n",
    "    dataset = dataset.select(range(MAX_SAMPLES)) # for N rows, there are 2*N answers for hallucination-detection LLM to classify (1 gt, 1 hallucinated answers)\n",
    "\n",
    "# --- Setup Model and Tokenizer\n",
    "print(f'Loading tokenizer for {MODEL_NAME}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, padding_side=\"left\") # was getting an error when trying to run inference on inputs with default padding_side = right\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # set pad token for batching if not present\n",
    "\n",
    "# --- Quantization Setup --- need to test this, and clean it up (^^ã‚ž\n",
    "print(f'Setting up {MODEL_NAME} with quantization: {QUANTIZATION_MODE}...')\n",
    "\n",
    "quantization_config = None\n",
    "if QUANTIZATION_MODE == '8bit_bnb':\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "elif QUANTIZATION_MODE == '4bit_bnb':\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "elif QUANTIZATION_MODE == '4bit_gptq':\n",
    "    quantization_config = GPTQConfig(bits=4, dataset='c4', tokenizer=tokenizer)\n",
    "else:\n",
    "    print(\"Loading uncompressed model\")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"quantization_config\": quantization_config,\n",
    "    \"device_map\": \"auto\",\n",
    "    \"attn_implementation\": \"eager\", #\"flash_attention_2\", ; doesn't work on Volta GPU (old) ; so ignoring this for now ; may revisit if I need to speed up inference\n",
    "    \"torch_dtype\": torch.float16, # torch.bfloat16 for ampere, maybe not best on older Volta GPU\n",
    "    \"trust_remote_code\": True\n",
    "}\n",
    "\n",
    "if QUANTIZATION_MODE == None:\n",
    "    model_kwargs.pop(\"quantization_config\")\n",
    "elif QUANTIZATION_MODE == \"4bit_gptq\":\n",
    "    model_kwards.pop(\"torch_dtype\")\n",
    "\n",
    "# --- Instantiate Model ---\n",
    "try:\n",
    "    if 'model' in globals():\n",
    "        print(\"Deleting existing global 'model' variable.\")\n",
    "        del globals()['model']\n",
    "    if 'classifier' in globals():\n",
    "        print(\"Deleting existing bloal 'classifier' variable.\")\n",
    "        del globals()['classifier']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Loading model: {MODEL_NAME}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
    "    print(f\"{MODEL_NAME} model loaded successfully!\")\n",
    "    #print(model.hf_device_map)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- Setup Pipeline ---\n",
    "classifier = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# --- Define Prompting Function ---\n",
    "# used in the paper\n",
    "original_system_prompt = \"\"\"\n",
    "You are an AI assistant with extensive knowledge in the field of medicine. I want you act as an answer judge. Given a question and an\n",
    "answer, your objective is to determine if the provided answer contains non-factual or hallucinated information. You SHOULD give your judgement based\n",
    "on the following hallucination types and the world knowledge.\n",
    "\n",
    "You are trying to determine if there is a factual contradiction between the answer and the world knowledge.\n",
    "Some information in the answer might be fabricated. Answer '0' if the answer is factual and '1' if the answer is hallucinated.\n",
    "You can categorize the answer as hallucinated if it falls in either of the category:\n",
    "\n",
    "Type: Misinterpretation of #Question#: These are hallucinated answers that misunderstands the question, leading to an off-topic or\n",
    "irrelevant response.\n",
    "Type: Incomplete Information: These are hallucinated answers that Point out what is not true without providing correct information.\n",
    "Type: Mechanism and Pathway Misattribution - These are hallucinated answer that falsely attribution of biological mechanisms,\n",
    "molecular pathways, or disease processes that contradicts established medical knowledge\n",
    "Type: Methodological and Evidence Fabrication - Inventing false research methods, statistical data, or specific clinical outcomes\n",
    "\n",
    "Do not return anything else, just the answer.\n",
    "Return just an integer value, '0' if the answer is factual and '1' if the answer is hallucinated. No letter or word, just the integer value.\n",
    "\"\"\"\n",
    "\n",
    "# modification on our part to allow model to refrain from making a classification if it is not sure\n",
    "not_sure_system_prompt = \"\"\"\n",
    "You are an AI assistant with extensive knowledge in the field of medicine. I want you act as an answer judge. Given a question, an\n",
    "answer, and relevant World Knowledge, determine if the provided answer is factual or contains hallucinated information based on the knowledge provided and established medical facts.\n",
    "\n",
    "- If the answer is factual and consistent with world knowledge, return '0'.\n",
    "- If the answer contains non-factual, hallucinated, or contradictory information based on the knowledge provided or established medical facts (e.g., Misinterpretation, Incomplete Info, Mechanism Misattribution, Fabrication), return '1'.\n",
    "- If you are genuinely uncertain or cannot make a definitive judgement based on the provided context and your medical knowledge, return '2'.\n",
    "\n",
    "Do not return anything else, just the integer value.\n",
    "\n",
    "Return '0' for factual, '1' for hallucinated, '2' for not sure.\n",
    "\n",
    "You can categorize the answer as hallucinated if it falls in either of the category:\n",
    "\n",
    "Type: Misinterpretation of #Question#: These are hallucinated answers that misunderstands the question, leading to an off-topic or\n",
    "irrelevant response.\n",
    "Type: Incomplete Information: These are hallucinated answers that Point out what is not true without providing correct information.\n",
    "Type: Mechanism and Pathway Misattribution - These are hallucinated answer that falsely attribution of biological mechanisms,\n",
    "molecular pathways, or disease processes that contradicts established medical knowledge\n",
    "Type: Methodological and Evidence Fabrication - Inventing false research methods, statistical data, or specific clinical outcomes\n",
    "\n",
    "Do not return anything else, just the answer.\n",
    "Return just an integer value, '0' if the answer is factual, '1' if the answer is hallucinated, '2' if not sure. No letter or word, just the integer value.\n",
    "\"\"\"\n",
    "\n",
    "not_sure_system_prompt2 = \"\"\"\n",
    "You are an AI judge with extensive knowledge in the field of medicine. I want you act as an answer judge. Given World Knowledge, a Question, and an Answer:\n",
    "- Return '0' if the Answer is factual based on the Knowledge and your understanding of medicine.\n",
    "- Return '1' if the Answer is hallucinated based on the Knowledge and/or your understanding of medicine.\n",
    "- Return '2' if you are unsure.\n",
    "\n",
    "You SHOULD give your judgement based on the following hallucination types and the world knowledge.\n",
    "You are trying to determine if there is a factual contradiction between the answer and the world knowledge.\n",
    "Some information in the answer might be fabricated. Answer '0' if the answer is factual, '1' if the answer is hallucinated, '2' if you are unsure.\n",
    "\n",
    "Output only the single digit 0, 1, or 2 and nothing else.\n",
    "\n",
    "You can categorize the answer as hallucinated if it falls in either of the category:\n",
    "\n",
    "Type: Misinterpretation of #Question#: These are hallucinated answers that misunderstands the question, leading to an off-topic or\n",
    "irrelevant response.\n",
    "Type: Incomplete Information: These are hallucinated answers that Point out what is not true without providing correct information.\n",
    "Type: Mechanism and Pathway Misattribution - These are hallucinated answer that falsely attribution of biological mechanisms,\n",
    "molecular pathways, or disease processes that contradicts established medical knowledge\n",
    "Type: Methodological and Evidence Fabrication - Inventing false research methods, statistical data, or specific clinical outcomes\n",
    "\n",
    "Do not return anything else, just the answer.\n",
    "Return just an integer value, '0' if the answer is factual, '1' if the answer is hallucinated, or '2'. No letter or word, just the integer value.\n",
    "\"\"\"\n",
    "\n",
    "def format_prompt_chatml(knowledge: str, question: str, answer: str) -> List[dict]:\n",
    "    few_shot_not_sure_user_content = f\"\"\"\n",
    "    World Knowledge: [Example Knowledge Snippet]\n",
    "    Question: [Example Question]\n",
    "    Answer: [Example Factual Answer]\n",
    "    Your Judgement: 0\n",
    "    \n",
    "    World Knowledge: [Example Knowledge Snippet 2]\n",
    "    Question: [Example Question 2]\n",
    "    Answer: [Example Hallucinated Answer]\n",
    "    Your Judgement: 1\n",
    "    \n",
    "    World Knowledge: [Example Knowledge Snippet 3 - where answer might be ambiguous or knowledge insufficient]\n",
    "    Question: [Example Question 3]\n",
    "    Answer: [Example Ambiguous Answer or Answer unrelated to Knowledge]\n",
    "    Your Judgement: 2\n",
    "    \n",
    "    --- Now your turn ---\n",
    "    World Knowledge: {knowledge}\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Return just '0' (factual), '1' (hallucinated), or '2' (not sure).\n",
    "    Your Judgement:\"\"\"\n",
    "\n",
    "    original_user_content = f\"\"\"\"\n",
    "    World Knowledge: {knowledge}\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "\n",
    "    Return just an integer value, '0' if the answer is factual, and '1' if the answer is hallucinated. No letter or word, just the integer value.\n",
    "    \n",
    "    Your Judgement:\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": original_system_prompt.strip()}, # remove leading/trailing whitespaces with .strip()\n",
    "        {\"role\": \"user\", \"content\": original_user_content.strip()} # original meaning, from the MedHallu paper\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "# --- Prompt Creation ---\n",
    "def prepare_prompts(dataset):\n",
    "    all_prompts = [] # all string prompts\n",
    "    #all_ground_truths = [] # corresponding labels for each prompt (0: truth, 1: hallucinated)\n",
    "    \n",
    "    print(\"Preparing prompts\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        knowledge = row[\"Knowledge\"]\n",
    "        question = row[\"Question\"]\n",
    "        hallucinated_answer = row[\"Hallucinated Answer\"]\n",
    "        ground_truth_answer = row[\"Ground Truth\"]\n",
    "    \n",
    "        # create prompts for hallucinated and ground truth answers\n",
    "        prompt_hallucinated = format_prompt_chatml(knowledge, question, hallucinated_answer)\n",
    "        prompt_truth = format_prompt_chatml(knowledge, question, ground_truth_answer)\n",
    "    \n",
    "        all_prompts.append(prompt_hallucinated)\n",
    "        #all_ground_truths.append(1)\n",
    "        all_prompts.append(prompt_truth)\n",
    "        #all_ground_truths.append(0)\n",
    "    print(\"Prompts are prepared.\")\n",
    "    return all_prompts\n",
    "\n",
    "all_prompts = prepare_prompts(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "dcb01657-9015-4704-b362-41bae760bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch inference on 2000 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [06:58<00:00,  1.67s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Inference ---\n",
    "# during inference, how much GPU RAM is used? can we measure peak/avg/min RAM usage? i.e. as in work by Ji Lin (MIT HAN Lab) on fitting CNNs in tiny MCUs\n",
    "def classify_med_answers(prompts):\n",
    "    print(f\"Starting batch inference on {len(prompts)} prompts...\")\n",
    "    outputs = []\n",
    "    num_batches=math.ceil(len(prompts) / BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        # show inference progress bar with tqdm\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Classifying Batches\", unit=\"batch\"):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = min(start_idx + BATCH_SIZE,len(all_prompts))\n",
    "            batch_prompts = prompts[start_idx:end_idx]\n",
    "            batch_output = classifier(batch_prompts,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      max_new_tokens=3, # should this be a hyperparam we include in a cfg file?\n",
    "                                      pad_token_id=tokenizer.pad_token_id,\n",
    "                                      eos_token_id=tokenizer.eos_token_id,\n",
    "                                      do_sample=False,\n",
    "                                      repetition_penalty=1.2) \n",
    "            outputs.extend(batch_output)\n",
    "    \n",
    "        # all at once\n",
    "        #outputs = classifier(\n",
    "        #    all_prompts,\n",
    "        #    max_new_tokens=3,\n",
    "        #    batch_size=BATCH_SIZE,\n",
    "        #    pad_token_id=tokenizer.pad_token_id,\n",
    "        #    eos_token_id=tokenizer.eos_token_id,\n",
    "        #    do_sample=False,\n",
    "        #    repetition_penalty=1.2\n",
    "        #)\n",
    "    print(\"Inference complete.\")\n",
    "    return outputs\n",
    "outputs = classify_med_answers(all_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f3c4de22-5def-48b1-ba9b-568457dc5830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Results.\n",
      "Results Processed.\n"
     ]
    }
   ],
   "source": [
    "# -- Process LLM Outputs -- \n",
    "predictions = []\n",
    "raw_outputs = []\n",
    "\n",
    "def parse_prediction(generated_text):\n",
    "    \"\"\"Extract the '0' or '1' from generated text, in case model does not listen to instructions and adds other tokens\"\"\"\n",
    "    text = generated_text.strip()\n",
    "    text_start = text[-10:]\n",
    "    #print(\"text start: \", text_start)\n",
    "    if '0' in text_start:\n",
    "        return 0\n",
    "    elif '1' in text_start:\n",
    "        return 1\n",
    "    elif '2' in text_start:\n",
    "        return 2\n",
    "    else:\n",
    "        #print(f\"Could not parse '0' or '1' from model output: {text}\")\n",
    "        return -1\n",
    "\n",
    "def extract_binary_predictions(outputs):\n",
    "    \"\"\"iterate through each output, filter out original prompt, extract binary prediction, and append to lists for predictions and raw model outputs\"\"\"\n",
    "    predictions = []\n",
    "    raw_outputs = []\n",
    "    print(\"Processing Results.\")\n",
    "    for i, output in enumerate(outputs):\n",
    "        full_chat = output[0]['generated_text'] # this INCLUDES the prompt ; we only want newly generated text\n",
    "        assistant_response_dict = full_chat[-1]\n",
    "        #print(assistant_response_dict)\n",
    "        model_response = None\n",
    "        if assistant_response_dict['role'] == 'assistant':\n",
    "            model_response = assistant_response_dict['content']\n",
    "        \n",
    "        pred = parse_prediction(model_response)\n",
    "        predictions.append(pred)\n",
    "        raw_outputs.append(model_response) # store the raw '!!!!!' or '0' or '1'\n",
    "    print(\"Results Processed.\")\n",
    "    return predictions, raw_outputs\n",
    "\n",
    "predictions, raw_outputs = extract_binary_predictions(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5c138d55-a7ab-4c24-bd36-02466b6c1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_invalid_pairs(pairs: list[tuple[int, int]]):\n",
    "    \"\"\"filter out any results where the model did not produce a valid output (i.e. anything other than 0 or 1)\"\"\"\n",
    "    valid_pairs = []\n",
    "    for pair in pairs:\n",
    "        if pair[1] == -1:\n",
    "            pass\n",
    "        else:\n",
    "            valid_pairs.append(pair)\n",
    "    return valid_pairs\n",
    "\n",
    "# --- Assemble final results ---\n",
    "all_ground_truths = [1,0] * dataset.num_rows # in prepare_prompts(), we alternate between adding prompts with hallucination and gt answers\n",
    "raw_gt_pred_pairs = list(zip(all_ground_truths, predictions))\n",
    "gt_pred_pairs = filter_invalid_pairs(raw_gt_pred_pairs)\n",
    "\n",
    "def compute_confusion_matrix_vals(gt_pred_pairs: list[tuple[int,int]]):\n",
    "    \"\"\"compute and return TP, FP, TN, FN from a list of (ground truth, prediction) pairs\"\"\"\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for (gt,pred) in gt_pred_pairs:\n",
    "        if pred == 1 and gt == 1:\n",
    "            TP += 1\n",
    "        elif pred == 1 and gt == 0:\n",
    "            FP += 1\n",
    "        elif pred == 0 and gt == 0:\n",
    "            TN += 1\n",
    "        elif pred == 0 and gt == 1:\n",
    "            FN += 1\n",
    "    return TP, FP, TN, FN\n",
    "    \n",
    "# --- Calculate Metrics ---\n",
    "accuracy = None\n",
    "precision = None\n",
    "recall = None\n",
    "f1 = None\n",
    "abstention_rate = None\n",
    "cm = None\n",
    "\n",
    "if len(gt_pred_pairs) > 0:\n",
    "    TP, FP, TN, FN = compute_confusion_matrix_vals(gt_pred_pairs)\n",
    "    cm = np.array([[TN, FP], [FN, TP]])\n",
    "    #cm = confusion_matrix(valid_gts, valid_preds, labels=[0,1])\n",
    "\n",
    "    # what proportion of answers did the LLM NOT classify?\n",
    "    total_valid_preds = len(gt_pred_pairs)\n",
    "    invalid_count = len(raw_gt_pred_pairs) - total_valid_preds\n",
    "    abstention_rate = invalid_count / len(raw_gt_pred_pairs)\n",
    "\n",
    "    # TBD: of the missed classifications, what proportion were easy/med/hard hallucinations? what proportion were type 1/2/3/4 hallucinations?\n",
    "\n",
    "    # TBD: of the classified answers, how did the LLM perform on easy/med/hard, type 1/2/3/4 hallucinations?\n",
    "\n",
    "    # accuracy, precision, recall, f1 score\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    valid_gts = [gt for (gt,pred) in gt_pred_pairs]\n",
    "    valid_preds = [pred for (gt,pred) in gt_pred_pairs]\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        valid_gts, valid_preds, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "else:\n",
    "    print(\"No valid predictions were made, skipping metric calculations.\")\n",
    "\n",
    "# --- Save Results ---\n",
    "results_df = []\n",
    "results_df.append({\n",
    "    'Baseline': {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'abstention_rate': abstention_rate}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d45fcc76-ed11-40c8-9b61-e4cf4be391f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 0), (0, 0), (1, 1), (0, 0), (0, 1), (0, 0), (0, 0), (1, 1)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_pred_pairs[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c1b8a5de-b3c5-499c-95a4-a7a121357b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Baseline': {'accuracy': 0.8023715415019763, 'precision': 0.6081081081081081, 'recall': 0.6818181818181818, 'f1': 0.6428571428571429, 'abstention_rate': 0.8735}}]\n"
     ]
    }
   ],
   "source": [
    "# can we write these results as a line in a text/csv/yaml file? is there a better way to track results i.e. incl exp name, use a config file, use tool(s) like Hydra, W&B?\n",
    "print(str(results_df))\n",
    "with open('results.txt', 'a') as results_file:\n",
    "    results_file.write(str(results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9a4fd6a3-d656-441e-82f7-fb15dd96e64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dp = ConfusionMatrixDisplay(cm)\n",
    "dp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f565f3aa-7e60-469e-a59d-1a676dce2b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66025c-41d5-45b0-aa28-9b63655261c4",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "48335133-43b8-4c0c-9175-41d86b897e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Question', 'Knowledge', 'Ground Truth', 'Difficulty Level', 'Hallucinated Answer', 'Category of Hallucination'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "medium Mechanism and Pathway Misattribution\n",
      "hard Incomplete Information\n",
      "hard Misinterpretation of #Question#\n",
      "easy Misinterpretation of #Question#\n"
     ]
    }
   ],
   "source": [
    "def check_dataset():\n",
    "  print(ds['train'])\n",
    "  i = 0\n",
    "  for row in ds['train']:\n",
    "    if i == 4:\n",
    "      break\n",
    "    question = row['Question']\n",
    "    hallucinated_answer = row['Hallucinated Answer']\n",
    "    ground_truth_answer = row['Ground Truth']\n",
    "    hal_difficulty = row[\"Difficulty Level\"]\n",
    "    hal_category = row[\"Category of Hallucination\"]\n",
    "    print(hal_difficulty, hal_category)\n",
    "    i += 1\n",
    "check_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb349efe-52b0-4659-92e7-0005c5b39113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experimenting with how I'd extract information related to the hallucination type and difficulty of answers in the dataset\n",
    "# --- the idea would be to correspond this information with raw_gt_pred_pairs and gt_pred_pairs so I can get more insights into (un)compressed model performance\n",
    "# --- i.e. not just summary info like acc.,prec.,rec.f1-score, but more detailed insights into how it performs with varying types/\"difficulty\" of hallucination\n",
    "\n",
    "#from datasets import Dataset\n",
    "#d = Dataset.from_dict({\"a\": [0, 1, 2], \"b\": [1, 1, 1], \"c\": [4, 4, 5]})\n",
    "#def p(example):\n",
    "#    example['gt_prompt'] = example['a'] + example['b']\n",
    "#d = d.map(p)\n",
    "#print(d)\n",
    "\n",
    "hal_difficulty = dataset['Difficulty Level']\n",
    "hal_category = dataset['Category of Hallucination']\n",
    "print(hal_difficulty[:3], hal_category[:3])\n",
    "\n",
    "hds = [None]*12\n",
    "hcs = [None]*12\n",
    "\n",
    "for idx, val in enumerate([1,0]*6):\n",
    "    print(idx, val)\n",
    "    if val == 1:\n",
    "        hds[idx] = dataset['Difficulty Level'][idx//2]\n",
    "        hcs[idx] = dataset['Category of Hallucination'][idx//2]\n",
    "print(hds, hcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371eb95-d865-40ff-bd13-74f841a758ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "[None] * 2 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2660a76-8d7a-4325-a794-796b2ec87dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_confusion_matrix_calculation():\n",
    "    # test 1\n",
    "    test_gt_pred_pairs = [(1,1), (1,1), (1, 1), (0,1), (0,0), (0,0), (1,0), (1,0), (1,0), (1,0), (1,0)]\n",
    "    TP, FP, TN, FN = compute_confusion_matrix_vals(test_gt_pred_pairs)\n",
    "    assert TP == 3 and FP == 1 and TN == 2 and FN == 5\n",
    "    \n",
    "    print(\"Test Passed (confusion matrix)\")\n",
    "test_confusion_matrix_calculation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
