{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb2d9fc-0f56-4582-b75c-8403cdd30022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  9 06:16:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:42:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             24W /  250W |       1MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d153ff6e-55f6-466c-8d50-d91ba5eaae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers accelerate datasets optimum peft auto-gptq bitsandbytes scikit-learn torch matplotlib tqdm --quiet\n",
    "!pip install flash-attn --no-build-isolation --quiet # for qwen2, newer nvidia gpus i.e. ampere and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96cea69-0725-40d6-891c-56327701f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UTAustin-AIHealth/MedHallu dataset (pqa_labeled)...\n",
      "Limiting dataset to 1000 samples for testing.\n",
      "Loading tokenizer for Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Qwen/Qwen2.5-7B-Instruct with quantization: None...\n",
      "Loading uncompressed model\n",
      "Deleting existing global 'model' variable.\n",
      "Loading model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a6e05ec37945b9be09cd639afc5b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen2.5-7B-Instruct model loaded successfully!\n",
      "Preparing prompts\n",
      "Prompts are prepared.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GPTQConfig, BitsAndBytesConfig\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, ConfusionMatrixDisplay #,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc # garbage collector interface\n",
    "import pprint\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-7B-Instruct' # try different models later\n",
    "DATASET_NAME = 'UTAustin-AIHealth/MedHallu'\n",
    "DATASET_CONFIG = 'pqa_labeled'\n",
    "BATCH_SIZE = 8\n",
    "MAX_SAMPLES = 1000\n",
    "QUANTIZATION_MODE = None\n",
    "OUTPUT_FILENAME = f'hallucination_results_{MODEL_NAME}_{QUANTIZATION_MODE}_{DATASET_CONFIG}.csv'\n",
    "\n",
    "# --- Load Dataset ---\n",
    "print(f'Loading {DATASET_NAME} dataset ({DATASET_CONFIG})...')\n",
    "ds = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "dataset = ds['train'] # use train split which has 1k labelled samples\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    print(f'Limiting dataset to {MAX_SAMPLES} samples for testing.')\n",
    "    dataset = dataset.select(range(MAX_SAMPLES)) # for N rows, there are 2*N answers for hallucination-detection LLM to classify (1 gt, 1 hallucinated answers)\n",
    "\n",
    "# --- Setup Model and Tokenizer\n",
    "print(f'Loading tokenizer for {MODEL_NAME}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, padding_side=\"left\") # was getting an error when trying to run inference on inputs with default padding_side = right\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # set pad token for batching if not present\n",
    "\n",
    "# --- Quantization Setup --- need to test this, and clean it up (^^ã‚ž\n",
    "print(f'Setting up {MODEL_NAME} with quantization: {QUANTIZATION_MODE}...')\n",
    "\n",
    "# get quantization config\n",
    "quantization_config = None\n",
    "if QUANTIZATION_MODE == '8bit_bnb':\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "elif QUANTIZATION_MODE == '4bit_bnb':\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=t.bfloat16\n",
    "    )\n",
    "elif QUANTIZATION_MODE == 'awq':\n",
    "    pass\n",
    "elif QUANTIZATION_MODE == '4bit_gptq':\n",
    "    quantization_config = GPTQConfig(bits=4, dataset='c4', tokenizer=tokenizer) # is the dataset ok to use? would it be better to use a medical dataset?\n",
    "else:\n",
    "    print(\"Loading uncompressed model\")\n",
    "\n",
    "# hyperparameters\n",
    "model_kwargs = {\n",
    "    \"quantization_config\": quantization_config,\n",
    "    \"device_map\": \"auto\",\n",
    "    \"attn_implementation\": \"eager\", #\"flash_attention_2\", ; doesn't work on Volta GPU (old) ; so ignoring this for now ; may revisit if I need to speed up inference\n",
    "    \"torch_dtype\": t.float16, # torch.bfloat16 for ampere, maybe not best on older Volta GPU\n",
    "    \"trust_remote_code\": True\n",
    "}\n",
    "\n",
    "if QUANTIZATION_MODE == None:\n",
    "    model_kwargs.pop(\"quantization_config\")\n",
    "elif QUANTIZATION_MODE == \"4bit_gptq\":\n",
    "    model_kwards.pop(\"torch_dtype\")\n",
    "\n",
    "# --- Instantiate Model ---\n",
    "model = None\n",
    "try:\n",
    "    if 'model' in globals():\n",
    "        print(\"Deleting existing global 'model' variable.\")\n",
    "        del globals()['model']\n",
    "    if 'classifier' in globals():\n",
    "        print(\"Deleting existing bloal 'classifier' variable.\")\n",
    "        del globals()['classifier']\n",
    "    gc.collect()\n",
    "    t.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Loading model: {MODEL_NAME}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
    "    print(f\"{MODEL_NAME} model loaded successfully!\")\n",
    "    #print(model.hf_device_map)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- Setup Pipeline ---\n",
    "classifier = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# --- Define Prompting Function ---\n",
    "# used in the paper\n",
    "original_system_prompt = \"\"\"\n",
    "You are an AI assistant with extensive knowledge in the field of medicine. I want you act as an answer judge. Given a question and an\n",
    "answer, your objective is to determine if the provided answer contains non-factual or hallucinated information. You SHOULD give your judgement based\n",
    "on the following hallucination types and the world knowledge.\n",
    "\n",
    "You are trying to determine if there is a factual contradiction between the answer and the world knowledge.\n",
    "Some information in the answer might be fabricated. Answer '0' if the answer is factual and '1' if the answer is hallucinated.\n",
    "You can categorize the answer as hallucinated if it falls in either of the category:\n",
    "\n",
    "Type: Misinterpretation of #Question#: These are hallucinated answers that misunderstands the question, leading to an off-topic or\n",
    "irrelevant response.\n",
    "Type: Incomplete Information: These are hallucinated answers that Point out what is not true without providing correct information.\n",
    "Type: Mechanism and Pathway Misattribution - These are hallucinated answer that falsely attribution of biological mechanisms,\n",
    "molecular pathways, or disease processes that contradicts established medical knowledge\n",
    "Type: Methodological and Evidence Fabrication - Inventing false research methods, statistical data, or specific clinical outcomes\n",
    "\n",
    "Do not return anything else, just the answer.\n",
    "Return just an integer value, '0' if the answer is factual and '1' if the answer is hallucinated. No letter or word, just the integer value.\n",
    "\"\"\"\n",
    "\n",
    "# modification on our part to allow model to refrain from making a classification if it is not sure\n",
    "not_sure_system_prompt = \"\"\"\n",
    "You are an AI assistant with extensive knowledge in the field of medicine. I want you act as an answer judge. Given a question, an\n",
    "answer, and relevant World Knowledge, determine if the provided answer is factual or contains hallucinated information based on the knowledge provided and established medical facts.\n",
    "\n",
    "- If the answer is factual and consistent with world knowledge, return '0'.\n",
    "- If the answer contains non-factual, hallucinated, or contradictory information based on the knowledge provided or established medical facts (e.g., Misinterpretation, Incomplete Info, Mechanism Misattribution, Fabrication), return '1'.\n",
    "- If you are genuinely uncertain or cannot make a definitive judgement based on the provided context and your medical knowledge, return '2'.\n",
    "\n",
    "Do not return anything else, just the integer value.\n",
    "\n",
    "Return '0' for factual, '1' for hallucinated, '2' for not sure.\n",
    "\n",
    "You can categorize the answer as hallucinated if it falls in either of the category:\n",
    "\n",
    "Type: Misinterpretation of #Question#: These are hallucinated answers that misunderstands the question, leading to an off-topic or\n",
    "irrelevant response.\n",
    "Type: Incomplete Information: These are hallucinated answers that Point out what is not true without providing correct information.\n",
    "Type: Mechanism and Pathway Misattribution - These are hallucinated answer that falsely attribution of biological mechanisms,\n",
    "molecular pathways, or disease processes that contradicts established medical knowledge\n",
    "Type: Methodological and Evidence Fabrication - Inventing false research methods, statistical data, or specific clinical outcomes\n",
    "\n",
    "Do not return anything else, just the answer.\n",
    "Return just an integer value, '0' if the answer is factual, '1' if the answer is hallucinated, '2' if not sure. No letter or word, just the integer value.\n",
    "\"\"\"\n",
    "\n",
    "not_sure_system_prompt2 = \"\"\"\n",
    "You are an AI judge with extensive knowledge in the field of medicine. I want you act as an answer judge. Given World Knowledge, a Question, and an Answer:\n",
    "- Return '0' if the Answer is factual based on the Knowledge and your understanding of medicine.\n",
    "- Return '1' if the Answer is hallucinated based on the Knowledge and/or your understanding of medicine.\n",
    "- Return '2' if you are unsure.\n",
    "\n",
    "You SHOULD give your judgement based on the following hallucination types and the world knowledge.\n",
    "You are trying to determine if there is a factual contradiction between the answer and the world knowledge.\n",
    "Some information in the answer might be fabricated. Answer '0' if the answer is factual, '1' if the answer is hallucinated, '2' if you are unsure.\n",
    "\n",
    "Output only the single digit 0, 1, or 2 and nothing else.\n",
    "\n",
    "You can categorize the answer as hallucinated if it falls in either of the category:\n",
    "\n",
    "Type: Misinterpretation of #Question#: These are hallucinated answers that misunderstands the question, leading to an off-topic or\n",
    "irrelevant response.\n",
    "Type: Incomplete Information: These are hallucinated answers that Point out what is not true without providing correct information.\n",
    "Type: Mechanism and Pathway Misattribution - These are hallucinated answer that falsely attribution of biological mechanisms,\n",
    "molecular pathways, or disease processes that contradicts established medical knowledge\n",
    "Type: Methodological and Evidence Fabrication - Inventing false research methods, statistical data, or specific clinical outcomes\n",
    "\n",
    "Do not return anything else, just the answer.\n",
    "Return just an integer value, '0' if the answer is factual, '1' if the answer is hallucinated, or '2'. No letter or word, just the integer value.\n",
    "\"\"\"\n",
    "\n",
    "def format_prompt_chatml(knowledge: str, question: str, answer: str, prompt_style=\"original\") -> List[dict]:\n",
    "    \"\"\"Put together world knowledge, a medical question, and a medical answer together in a prompt according to requested prompt_style\"\"\"\n",
    "    \n",
    "    few_shot_not_sure_user_content = f\"\"\"\n",
    "    World Knowledge: [Example Knowledge Snippet]\n",
    "    Question: [Example Question]\n",
    "    Answer: [Example Factual Answer]\n",
    "    Your Judgement: 0\n",
    "    \n",
    "    World Knowledge: [Example Knowledge Snippet 2]\n",
    "    Question: [Example Question 2]\n",
    "    Answer: [Example Hallucinated Answer]\n",
    "    Your Judgement: 1\n",
    "    \n",
    "    World Knowledge: [Example Knowledge Snippet 3 - where answer might be ambiguous or knowledge insufficient]\n",
    "    Question: [Example Question 3]\n",
    "    Answer: [Example Ambiguous Answer or Answer unrelated to Knowledge]\n",
    "    Your Judgement: 2\n",
    "    \n",
    "    --- Now your turn ---\n",
    "    World Knowledge: {knowledge}\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Return just '0' (factual), '1' (hallucinated), or '2' (not sure).\n",
    "    Your Judgement:\"\"\"\n",
    "\n",
    "    original_user_content = f\"\"\"\"\n",
    "    World Knowledge: {knowledge}\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "\n",
    "    Return just an integer value, '0' if the answer is factual, and '1' if the answer is hallucinated. No letter or word, just the integer value.\n",
    "    \n",
    "    Your Judgement:\"\"\"\n",
    "\n",
    "    user_prompt = None\n",
    "    if prompt_style == \"few_shot_not_sure\":\n",
    "        user_prompt = few_shot_not_sure_user_content\n",
    "    elif prompt_style == \"original\":\n",
    "        user_prompt = original_user_content\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": original_system_prompt.strip()}, # remove leading/trailing whitespaces with .strip()\n",
    "        {\"role\": \"user\", \"content\": original_user_content.strip()} # original meaning, from the MedHallu paper\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "# --- Prompt Creation ---\n",
    "def prepare_prompts(dataset):\n",
    "    all_prompts = [] # all string prompts\n",
    "    #all_ground_truths = [] # corresponding labels for each prompt (0: truth, 1: hallucinated)\n",
    "    \n",
    "    print(\"Preparing prompts\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        knowledge = row[\"Knowledge\"]\n",
    "        question = row[\"Question\"]\n",
    "        hallucinated_answer = row[\"Hallucinated Answer\"]\n",
    "        ground_truth_answer = row[\"Ground Truth\"]\n",
    "    \n",
    "        # create prompts for hallucinated and ground truth answers\n",
    "        prompt_hallucinated = format_prompt_chatml(knowledge, question, hallucinated_answer)\n",
    "        prompt_truth = format_prompt_chatml(knowledge, question, ground_truth_answer)\n",
    "    \n",
    "        all_prompts.append(prompt_hallucinated)\n",
    "        #all_ground_truths.append(1)\n",
    "        all_prompts.append(prompt_truth)\n",
    "        #all_ground_truths.append(0)\n",
    "    print(\"Prompts are prepared.\")\n",
    "    return all_prompts\n",
    "\n",
    "all_prompts = prepare_prompts(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb01657-9015-4704-b362-41bae760bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch inference on 2000 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Classifying Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [07:29<00:00,  1.80s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m t\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats()\n\u001b[1;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m classify_med_answers(all_prompts)\n\u001b[0;32m---> 40\u001b[0m peak_memory_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_allocated()\n\u001b[1;32m     41\u001b[0m peak_memory_gb \u001b[38;5;241m=\u001b[39m peak_memory_btes \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m) \u001b[38;5;66;03m# 2^30B in one GB\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeak GPU Memory Allocated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeak_memory_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# -- Inference ---\n",
    "# during inference, how much GPU RAM is used? can we measure peak/avg/min RAM usage? i.e. as in work by Ji Lin (MIT HAN Lab) on fitting CNNs in tiny MCUs\n",
    "def classify_med_answers(prompts):\n",
    "    print(f\"Starting batch inference on {len(prompts)} prompts...\")\n",
    "    outputs = []\n",
    "    num_batches=math.ceil(len(prompts) / BATCH_SIZE)\n",
    "    with t.no_grad():\n",
    "        # show inference progress bar with tqdm\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Classifying Batches\", unit=\"batch\"):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = min(start_idx + BATCH_SIZE,len(all_prompts))\n",
    "            batch_prompts = prompts[start_idx:end_idx]\n",
    "            batch_output = classifier(batch_prompts,\n",
    "                                      #batch_size=BATCH_SIZE # redundant\n",
    "                                      max_new_tokens=3, # should this be a hyperparam we include in a cfg file?\n",
    "                                      pad_token_id=tokenizer.pad_token_id,\n",
    "                                      eos_token_id=tokenizer.eos_token_id,\n",
    "                                      do_sample=False,\n",
    "                                      repetition_penalty=1.2) \n",
    "            outputs.extend(batch_output)\n",
    "    \n",
    "        # all at once\n",
    "        #outputs = classifier(\n",
    "        #    all_prompts,\n",
    "        #    max_new_tokens=3,\n",
    "        #    batch_size=BATCH_SIZE,\n",
    "        #    pad_token_id=tokenizer.pad_token_id,\n",
    "        #    eos_token_id=tokenizer.eos_token_id,\n",
    "        #    do_sample=False,\n",
    "        #    repetition_penalty=1.2\n",
    "        #)\n",
    "    print(\"Inference complete.\")\n",
    "    return outputs\n",
    "\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "t.cuda.reset_peak_memory_stats()\n",
    "outputs = classify_med_answers(all_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9eae3d5-c366-42f5-b692-09d4d51af687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU Memory Allocated: 14.810412883758545 GB\n"
     ]
    }
   ],
   "source": [
    "## --- Profiling GPU Usage ---\n",
    "peak_memory_bytes = t.cuda.max_memory_allocated()\n",
    "peak_memory_gb = peak_memory_bytes / (2**30) # 2^30B in one GB\n",
    "print(f\"Peak GPU Memory Allocated: {peak_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8352f8-fa06-4df9-9234-54129e5ac7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.193614482879639\n",
      "14.810412883758545\n",
      "15.85546875\n"
     ]
    }
   ],
   "source": [
    "print(t.cuda.memory_allocated() / 2**30)\n",
    "print(t.cuda.max_memory_allocated() / 2**30)\n",
    "print(t.cuda.memory_reserved() / 2**30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c4de22-5def-48b1-ba9b-568457dc5830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Results.\n",
      "Results Processed.\n"
     ]
    }
   ],
   "source": [
    "# -- Process LLM Outputs -- \n",
    "predictions = []\n",
    "raw_outputs = []\n",
    "\n",
    "def parse_prediction(generated_text):\n",
    "    \"\"\"Extract the '0' or '1' from generated text, in case model does not listen to instructions and adds other tokens\"\"\"\n",
    "    text = generated_text.strip()\n",
    "    text_start = text[-10:]\n",
    "    #print(\"text start: \", text_start)\n",
    "    if '0' in text_start:\n",
    "        return 0\n",
    "    elif '1' in text_start:\n",
    "        return 1\n",
    "    elif '2' in text_start:\n",
    "        return 2\n",
    "    else:\n",
    "        #print(f\"Could not parse '0' or '1' from model output: {text}\")\n",
    "        return -1\n",
    "\n",
    "def extract_binary_predictions(outputs):\n",
    "    \"\"\"iterate through each output, filter out original prompt, extract binary prediction, and append to lists for predictions and raw model outputs\"\"\"\n",
    "    predictions = []\n",
    "    raw_outputs = []\n",
    "    print(\"Processing Results.\")\n",
    "    for i, output in enumerate(outputs):\n",
    "        model_response = None\n",
    "        try:\n",
    "            full_chat = output[0]['generated_text'] # this INCLUDES the prompt ; we only want newly generated text\n",
    "            assistant_response_dict = full_chat[-1] # full_chat[0]: system, full_chat[1]: user, full_chat[2]: assistant\n",
    "            #print(assistant_response_dict)\n",
    "            model_response = None\n",
    "            if assistant_response_dict['role'] == 'assistant':\n",
    "                model_response = assistant_response_dict['content']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        pred = parse_prediction(model_response)\n",
    "        predictions.append(pred)\n",
    "        raw_outputs.append(model_response) # store the raw '!!!!!' or '0' or '1'\n",
    "    print(\"Results Processed.\")\n",
    "    return predictions, raw_outputs\n",
    "\n",
    "predictions, raw_outputs = extract_binary_predictions(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c138d55-a7ab-4c24-bd36-02466b6c1b4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# something to think about.. what is the goal here? if we have an assistant AI answering medical questions,\n",
    "# if we verify answers with an LLM-checker, we'd want that LLM-checker to be compact so deployment is easy\n",
    "# and we'd want it to be good and identifying the hallucinations so we can weed them out\n",
    "# and protect relevant stakeholders (i.e. doctors, laypeople) from misleading information\n",
    "# for developers, if we can get insights into types of answers/hallucinations that compressed models work best/worst with\n",
    "# this could help them inform strategies for deploying an LLM-checker\n",
    "# i.e. \"for our use case, it's most important to avoid B type hallucinations, or that's the most commong type that \n",
    "# could come up in our work ; we have X physical constraints for the device we have access to\"\n",
    "# so for them, perhaps we'd go with 1B awq_4bit quantized model. granted, model's move fast i.e.\n",
    "# what's new today is old tomorrow ; so at the very least, we'd be contributed a codebase that\n",
    "# developers can use to easily swap in different models, to quickly test what's best for their use case\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "\n",
    "@dataclass\n",
    "class HallucinationMetrics:\n",
    "    difficulty_metrics: Dict[str, float]\n",
    "    category_metrics: Dict[str, float]\n",
    "    overall_metrics: Dict[str, float]\n",
    "    abstention_analysis: Dict[str, float]\n",
    "\n",
    "def get_hallucination_info(info_type: str):\n",
    "    \"\"\"\n",
    "    construct lists for the type and difficulty of hallucinated answers in all_prompts\n",
    "    all_ground_truths:        [    1,     0,        1,    0, ...]\n",
    "    hallucination_difficulty: [\"easy\", None, \"medium\", None, ...]\n",
    "    \"\"\"\n",
    "    hallucination_difficulty = [None]*dataset.num_rows*2\n",
    "    hallucination_category = [None]*dataset.num_rows*2\n",
    "    for idx, val in enumerate(all_ground_truths):\n",
    "        if val == 1:\n",
    "            hallucination_difficulty[idx] = dataset['Difficulty Level'][idx//2]\n",
    "            hallucination_category[idx] = dataset['Category of Hallucination'][idx//2]\n",
    "\n",
    "    if info_type == \"difficulty\":\n",
    "        return hallucination_difficulty\n",
    "    elif info_type == \"category\":\n",
    "        return hallucination_category\n",
    "\n",
    "# --- Assemble final results ---\n",
    "# example\n",
    "# all_ground_truths:              [    1,     0,     1,     0]\n",
    "# raw_gt_pred_pairs:             [(1,0), (0,-1), (1,1), (0,0)]\n",
    "# raw_hallucination_difficulty: [\"hard\",   None,\"easy\",  None]\n",
    "# raw_hallucination_category:  [   \"A\",    None,   \"B\",  None]\n",
    "\n",
    "# valid_idxs: [0,2,3] (explicit) ; [1, 0, 1, 1] (boolean mask) ; we'll go with the explicit version\n",
    "# gt_pred_pairs:         [(1,0),  (1,1),   (0,0)]\n",
    "# valid_hal_difficulty: [\"hard\", \"easy\",   None]\n",
    "# valid_hal_category:  [    \"A\",   \"B\",   None]\n",
    "\n",
    "all_ground_truths = [1,0] * dataset.num_rows # in prepare_prompts(), we alternate between adding prompts with hallucination and gt answers\n",
    "\n",
    "raw_gt_pred_pairs = t.tensor(list(zip(all_ground_truths, predictions))) # tensor allows for easy selection from a list of desired idxs\n",
    "raw_hallucination_difficulty = get_hallucination_info(\"difficulty\") # can't convert to tensor b/c has strings\n",
    "raw_hallucination_category = get_hallucination_info(\"category\")\n",
    "\n",
    "valid_idxs = [idx for idx, (gt,pred) in enumerate(raw_gt_pred_pairs) if pred != -1] # explicit [0, 2, 3, 4, 7, 110]\n",
    "gt_pred_pairs = raw_gt_pred_pairs[valid_idxs] #filter_invalid_pairs(raw_gt_pred_pairs)\n",
    "valid_hal_difficulty = [raw_hallucination_difficulty[valid_idx] for valid_idx in valid_idxs]\n",
    "valid_hal_category = [raw_hallucination_category[valid_idx] for valid_idx in valid_idxs]\n",
    "\n",
    "# quetsions we want to ask\n",
    "# raw information : what answers were NOT classified? \n",
    "#                   of those, what proportion were hallucinated? \n",
    "#                   of those, what proportion were easy/med/diff, type 1/2/3/4?\n",
    "\n",
    "# valid information : of the classified answers, how reliable are the classifications? (acc/rec/prec/f1)\n",
    "#                     for gt hallucinations, were are particular hallucination type/difficulty that the model did well/struggled on?\n",
    "\n",
    "def compute_confusion_matrix_vals(gt_pred_pairs: list[tuple[int,int]]):\n",
    "    \"\"\"compute and return TP, FP, TN, FN from a list of (ground truth, prediction) pairs\"\"\"\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for (gt,pred) in gt_pred_pairs:\n",
    "        if pred == 1 and gt == 1:\n",
    "            TP += 1\n",
    "        elif pred == 1 and gt == 0:\n",
    "            FP += 1\n",
    "        elif pred == 0 and gt == 0:\n",
    "            TN += 1\n",
    "        elif pred == 0 and gt == 1:\n",
    "            FN += 1\n",
    "    return TP, FP, TN, FN\n",
    "    \n",
    "# --- Calculate Metrics ---\n",
    "accuracy = None\n",
    "precision = None\n",
    "recall = None\n",
    "f1 = None\n",
    "abstention_rate = None\n",
    "cm = None\n",
    "\n",
    "if len(gt_pred_pairs) > 0:\n",
    "    TP, FP, TN, FN = compute_confusion_matrix_vals(gt_pred_pairs)\n",
    "    cm = np.array([[TN, FP], [FN, TP]])\n",
    "    #cm = confusion_matrix(valid_gts, valid_preds, labels=[0,1])\n",
    "\n",
    "    # what proportion of answers did the LLM NOT classify?\n",
    "    total_valid_preds = len(gt_pred_pairs)\n",
    "    invalid_count = len(raw_gt_pred_pairs) - total_valid_preds\n",
    "    abstention_rate = invalid_count / len(raw_gt_pred_pairs)\n",
    "\n",
    "    # TBD: of the missed classifications, what proportion were easy/med/hard hallucinations? what proportion were type 1/2/3/4 hallucinations?\n",
    "\n",
    "    # TBD: of the classified answers, how did the LLM perform on easy/med/hard, type 1/2/3/4 hallucinations?\n",
    "\n",
    "    # accuracy, precision, recall, f1 score\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    valid_gts = [gt for (gt,pred) in gt_pred_pairs]\n",
    "    valid_preds = [pred for (gt,pred) in gt_pred_pairs]\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        valid_gts, valid_preds, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "else:\n",
    "    print(\"No valid predictions were made, skipping metric calculations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1b8a5de-b3c5-499c-95a4-a7a121357b3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Baseline': {'accuracy': 0.8215, 'precision': 0.8277268093781855, 'recall': 0.812, 'f1': 0.8197879858657244, 'abstention_rate': 0.0, 'gpu_peak_memory_gb': 14.810412883758545}}]\n"
     ]
    }
   ],
   "source": [
    "# --- Save Results ---\n",
    "# can we write these results as a line in a text/csv/yaml file?\n",
    "# is there a better way to track results i.e. incl exp name, use a config file, use tool(s) like Hydra, W&B?\n",
    "results_df = []\n",
    "results_df.append({\n",
    "    'Baseline': {'accuracy': accuracy, \n",
    "                 'precision': precision,\n",
    "                 'recall': recall, \n",
    "                 'f1': f1,\n",
    "                 'abstention_rate': abstention_rate, \n",
    "                 'gpu_peak_memory_gb': peak_memory_gb}\n",
    "})\n",
    "\n",
    "print(str(results_df))\n",
    "with open('results.txt', 'w') as results_file:\n",
    "    results_file.write(str(results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a4fd6a3-d656-441e-82f7-fb15dd96e64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dp = ConfusionMatrixDisplay(cm)\n",
    "dp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e3467-bb34-40ad-8799-92972452f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Further Analysis of Classifications on Hallucinated Answers --- \n",
    "def calculate_metrics_for_subset(gts, preds, pos_label=1, zero_division=0):\n",
    "    \"\"\"calculate metrics for a given set of ground truth and prediction labels\"\"\"\n",
    "    if len(gts) == 0:\n",
    "        return {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0, 'support': 0}\n",
    "\n",
    "    gt_pred_pairs = list(zip(gts, preds))\n",
    "    TP, FP, TN, FN = compute_confusion_matrix_vals(gt_pred_pairs)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    precision, recall, f1, support = precision_recall_f1_support(\n",
    "        gts, preds, average='binary', pos_label=pos_label, zero_division=zero_divison\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': len(gts)\n",
    "    }\n",
    "\n",
    "# practically, there's not much we can do if the LLM does not produce a classification\n",
    "# BUT, if we have a group of LLMs, and we know certain abstain from certain types/difficulties,\n",
    "# that could inform a decision to go with a certain LLM\n",
    "\n",
    "# type_1_matrics = calculate_metrics_for_subset() # and then for each type, we could look at proportion of easy/med/hard answers?\n",
    "# type_2_metrics = calculate_metrics_for_subset()\n",
    "# type_3_metrics = calculate_metrics_for_subset()\n",
    "# type_4_metrics = calculate_metrics_for_subset()\n",
    "\n",
    "def analyze_abstentions(invalid_idxs, raw_difficulty, raw_category):\n",
    "    # what kinds of medical answers were skipped?\n",
    "    # are we avoiding difficult cases? specific categories?\n",
    "    num_abstained = len(invalid_idxs)\n",
    "    abstained_gts = None\n",
    "    abstained_difficulties = None\n",
    "    abstained_categories = None\n",
    "\n",
    "def analyze_performance_by_group(valid_gts, valid_preds, grouping_factor, group_type_name=\"group\"):\n",
    "    pass\n",
    "\n",
    "    results = {}\n",
    "    df = pd.DataFrame({\n",
    "        'gts': valid_gts,\n",
    "        'preds': valid_preds,\n",
    "        'group': grouping_factor\n",
    "    })\n",
    "    results['overall'] = calculate_metrics_for_subset(df['gts'].to_list(), df['preds'].to_list())\n",
    "\n",
    "    # now focus on the hallucinated answers\n",
    "    hal_df = df[df['gts'] == 1].copy() # deep copy ; changes to copy do not affect original\n",
    "    hal_types = hal_df['group'].unique()\n",
    "    hal_types = [t for t in hal_types if t is not None]\n",
    "    grouped_metrics = {}\n",
    "    for group_name in hal_types:\n",
    "        group_df = hal_df[hal_df['group'] == group_name]\n",
    "        if not group_df.empty:\n",
    "            group_gts = group_df['gts'].to_list()\n",
    "            group_preds = group_df['preds'].to_list()\n",
    "            grouped_metrics[group_name] = calculate_metrics_for_subset(group_gts, group_preds)\n",
    "        else:\n",
    "            grouped_metrics[group_name] = calculate_metrics_for_subset([], [])\n",
    "    results[f'hallucination_performance_by_{group_name}'] = grouped_metrics\n",
    "    \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11320c24-0b9d-427a-aabd-dbbdabd59ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[idx for idx, (gt,pred) in enumerate(raw_gt_pred_pairs) if pred != -1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e88f8c-7a71-4eb7-9a15-c46217e12e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_idxs[:10])\n",
    "vhd = [raw_hallucination_difficulty[valid_idx] for valid_idx in valid_idxs]\n",
    "print(vhd[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ea5ec-93b0-4f32-a518-e33de90e9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.tensor([10., 20., 30., 40., 50., 60.])\n",
    "a[[0, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d838517-b8b9-4442-9467-1e4d5ce9712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tensor(list(zip(all_ground_truths, predictions))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fcc76-ed11-40c8-9b61-e4cf4be391f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gt_pred_pairs[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565f3aa-7e60-469e-a59d-1a676dce2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66025c-41d5-45b0-aa28-9b63655261c4",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48335133-43b8-4c0c-9175-41d86b897e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset():\n",
    "  print(ds['train'])\n",
    "  i = 0\n",
    "  for row in ds['train']:\n",
    "    if i == 4:\n",
    "      break\n",
    "    question = row['Question']\n",
    "    hallucinated_answer = row['Hallucinated Answer']\n",
    "    ground_truth_answer = row['Ground Truth']\n",
    "    hal_difficulty = row[\"Difficulty Level\"]\n",
    "    hal_category = row[\"Category of Hallucination\"]\n",
    "    print(hal_difficulty, hal_category)\n",
    "    i += 1\n",
    "check_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb349efe-52b0-4659-92e7-0005c5b39113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experimenting with how I'd extract information related to the hallucination type and difficulty of answers in the dataset\n",
    "# --- the idea would be to correspond this information with raw_gt_pred_pairs and gt_pred_pairs so I can get more insights into (un)compressed model performance\n",
    "# --- i.e. not just summary info like acc.,prec.,rec.f1-score, but more detailed insights into how it performs with varying types/\"difficulty\" of hallucination\n",
    "\n",
    "#from datasets import Dataset\n",
    "#d = Dataset.from_dict({\"a\": [0, 1, 2], \"b\": [1, 1, 1], \"c\": [4, 4, 5]})\n",
    "#def p(example):\n",
    "#    example['gt_prompt'] = example['a'] + example['b']\n",
    "#d = d.map(p)\n",
    "#print(d)\n",
    "\n",
    "hal_difficulty = dataset['Difficulty Level']\n",
    "hal_category = dataset['Category of Hallucination']\n",
    "print(hal_difficulty[:3], hal_category[:3])\n",
    "\n",
    "hds = [None]*12\n",
    "hcs = [None]*12\n",
    "\n",
    "for idx, val in enumerate([1,0]*6):\n",
    "    print(idx, val)\n",
    "    if val == 1:\n",
    "        hds[idx] = dataset['Difficulty Level'][idx//2]\n",
    "        hcs[idx] = dataset['Category of Hallucination'][idx//2]\n",
    "print(hds, hcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371eb95-d865-40ff-bd13-74f841a758ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "[None] * 2 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2660a76-8d7a-4325-a794-796b2ec87dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_confusion_matrix_calculation():\n",
    "    # test 1\n",
    "    test_gt_pred_pairs = [(1,1), (1,1), (1, 1), (0,1), (0,0), (0,0), (1,0), (1,0), (1,0), (1,0), (1,0)]\n",
    "    TP, FP, TN, FN = compute_confusion_matrix_vals(test_gt_pred_pairs)\n",
    "    assert TP == 3 and FP == 1 and TN == 2 and FN == 5\n",
    "    \n",
    "    print(\"Test Passed (confusion matrix)\")\n",
    "test_confusion_matrix_calculation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
